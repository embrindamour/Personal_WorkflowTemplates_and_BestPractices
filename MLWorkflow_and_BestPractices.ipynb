{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c233b2-55db-4d24-956c-d12d03cd22cd",
   "metadata": {},
   "source": [
    "# Typical Project Workflow and Best Practices\n",
    "### Pandas, Numpy, Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962952cd-5aee-48dc-82fe-72256a5ce7dc",
   "metadata": {},
   "source": [
    "### 1. Analyze the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aef640-a2dc-4adb-9fb0-db1c0a70292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DECISION TREE 1: IDENTIFY PROBLEM TYPE\n",
    "├── Target is continuous (price, temperature, count) → REGRESSION\n",
    "│   ├── Evaluation metric mentioned?\n",
    "│   │   ├── MAE → Use mean_absolute_error\n",
    "│   │   ├── MSE/RMSE → Use mean_squared_error\n",
    "│   │   ├── R² → Use r2_score\n",
    "│   │   └── Not mentioned → Assume MAE or RMSE\n",
    "│   └── Default models: RandomForest, GradientBoosting\n",
    "│\n",
    "└── Target is categorical (yes/no, classes) → CLASSIFICATION\n",
    "    ├── Binary (2 classes) → BINARY CLASSIFICATION\n",
    "    │   ├── Metric: accuracy, precision, recall, F1, AUC\n",
    "    │   └── Models: LogisticRegression, RandomForest, GradientBoosting\n",
    "    └── Multi-class (3+ classes) → MULTICLASS CLASSIFICATION\n",
    "        ├── Metric: accuracy, weighted F1\n",
    "        └── Models: RandomForest, GradientBoosting\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b4181-c023-4d4a-b5e2-c4698950649c",
   "metadata": {},
   "source": [
    "### 2. Load data & Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c86860-0613-4120-96cd-6c4429f62d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2.1: Data Loading Strategy\n",
    "2.2: Quick EDA Checks\n",
    "├── Missing Values: <5%, 5-30%, >30%\n",
    "├── Data Types: numerical, categorical, dates\n",
    "├── Target Analysis: outliers, imbalance\n",
    "└── Feature Types: low/high cardinality\n",
    "'''\n",
    "# 2.1: Load data\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df = pd.read_excel('dataset.csv', parse_dates=['date', 'timestamp'])\n",
    "\n",
    "# LOAD MULTIPLE FILES (cars_0.csv, cars_1.csv, etc.)\n",
    "dfs = []\n",
    "for i in range(4):  # cars_0 through cars_3\n",
    "    df = pd.read_csv(f'cars_{i}.csv')\n",
    "    dfs.append(df)\n",
    "\n",
    "train = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"✓ Combined {len(dfs)} files into {train.shape[0]} rows\")\n",
    "\n",
    "#2.2: Quick EDA\n",
    "print(train.shape, val.shape, test.shape)\n",
    "\n",
    "print(train.head(10)) \n",
    "print(train.sample(10, random_state=42))\n",
    "\n",
    "print(train.dtypes)\n",
    "print(train.info())\n",
    "print(train.describe())\n",
    "print(train.isnull().sum())\n",
    "\n",
    "n_duplicates = train.duplicated().sum()\n",
    "duplicates = train[train.duplicated(keep=False)]\n",
    "print(duplicates.head(10))\n",
    "\n",
    "# more on missing vals\n",
    "\n",
    "cols_with_missing = train.columns[train.isnull().any()].tolist()\n",
    "\n",
    "rows_with_missing = train.isnull().any(axis=1).sum()\n",
    "\n",
    "# summary stats\n",
    "print(train.describe())\n",
    "print(train.describe(include='all')) # include categorical cols\n",
    "\n",
    "# stats on target col (regression)\n",
    "print(train[target_col].describe()) # basic stats\n",
    "\n",
    "print(f\"Skewness: {train[target_col].skew():.3f}\") # target col distribution\n",
    "print(f\"Kurtosis: {train[target_col].kurt():.3f}\")\n",
    "\n",
    "Q1 = train[target_col].quantile(0.25) # outlier check\n",
    "Q3 = train[target_col].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outliers = train[(train[target_col] < lower_bound) | (train[target_col] > upper_bound)]\n",
    "outliers.head()\n",
    "percent_of_outliers = len(outliers)/len(train)*100\n",
    "\n",
    "print(f\"Zero values: {(train[target_col] == 0).sum()}\")\n",
    "print(f\"Negative values: {(train[target_col] < 0).sum()}\")\n",
    "\n",
    "# stats on target col (classification)\n",
    "n_classes = train[target_col].nunique()\n",
    "counts_perclass = train[target_col].value_counts()\n",
    "\n",
    "minority_class_percent = counts.min() / counts.sum() * 100\n",
    "print(f\"Minority class: {minority_class_percent:.1f}%\")\n",
    "if minority_class_pct < 30:\n",
    "    print(\"IMBALANCED - Use class_weight='balanced'\")\n",
    "else:\n",
    "    print(\"Balanced\")\n",
    "\n",
    "# Breakdown by type\n",
    "numeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "cat_cols = train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "date_cols = train.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "# stats on numerical cols\n",
    "print(train[numerical_cols].describe())\n",
    "n_unique = train[col].nunique() # in case numerical is actually categorical -- n_unique <=10\n",
    "neg_count = (train[col] < 0).sum() # check for neg vals\n",
    "\n",
    "if train[col].std() == 0: # check for constant cols\n",
    "        print(f\"⚠️ {col} is constant (std=0) - Consider dropping\")\n",
    "\n",
    "# stats on categorical cols\n",
    "\n",
    "for col in cat_cols: # check for natural order and cardinality\n",
    "    n_unique = train[col].nunique()\n",
    "    print(f\"{col}: {n_unique} unique values\")\n",
    "    print(f\"Value Counts:{train[col].value_counts()}\")\n",
    "\n",
    "binary_cols = []\n",
    "low_card_cols = []\n",
    "high_card_cols = []\n",
    "\n",
    "for col in cat_cols:\n",
    "    n_unique = train[col].nunique()\n",
    "    if n_unique == 2:\n",
    "        binary_cols.append(col)\n",
    "    elif n_unique <= 10:\n",
    "        low_card_cols.append(col)\n",
    "    else:\n",
    "        high_card_cols.append(col)\n",
    "\n",
    "# stats on date cols\n",
    "print(f\"Date range: {train[date_col].min()} to {train[date_col].max()}\")\n",
    "train['year'] = train[date_col].dt.year\n",
    "train['month'] = train[date_col].dt.month\n",
    "train['day'] = train[date_col].dt.day\n",
    "\n",
    "print(train['year'].value_counts().sort_index()) # check distribution\n",
    "print(train['month'].value_counts().sort_index())\n",
    "\n",
    "# stats on correlations\n",
    "correlations = train[numeric_cols].corr()[target_col].sort_values(ascending=False) # WITH TARGET\n",
    "print(correlations)\n",
    "\n",
    "corr_matrix = train[numeric_cols].corr() # WITH EACHOTHER (multicollinearity)\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e38725-170b-4734-8eb6-d411729fd2c8",
   "metadata": {},
   "source": [
    "### 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1bd8f-77d7-4219-a9a4-63f44f0f5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3.1: Handle missing values\n",
    "├── <5% missing → simple imputation: mean/median/mode\n",
    "├── 5-30% missing → KNN imputation, group-wise mean imputation, or drop\n",
    "└── >30% missing → DROP column\n",
    "\n",
    "3.2: Categorical Encoding\n",
    "├── 2 values → Label encoding\n",
    "├── 3-10 values → Ordinal or One-hot\n",
    "└── >10 values → Label encoding (trees) or Target encoding\n",
    "\n",
    "3.3: Scaling Strategy\n",
    "├── Tree-based → NO scaling needed\n",
    "├── Linear models → StandardScaler REQUIRED\n",
    "└── Neural nets → MinMaxScaler or StandardScaler\n",
    "\n",
    "3.4: Feature Engineering\n",
    "├── Time available >5 min → Try interactions\n",
    "└── <5 min → Skip\n",
    "'''\n",
    "##########################\n",
    "# 3.1: MISSING VALS\n",
    "##########################\n",
    "# IF <5% MISSING & NUMERICAL COL: Simple median imputation (robust to outliers)\n",
    "numeric_cols = train.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if train[col].isnull().sum() > 0:\n",
    "        median = train[col].median()\n",
    "        train[col].fillna(median, inplace=True)\n",
    "        val[col].fillna(median, inplace = True)\n",
    "        test[col].fillna(median, inplace = True)\n",
    "        \n",
    "# IF <5% MISSING & CATEGORICAL COL: Simple mode imputation (robust to outliers)\n",
    "mode = train['category'].mode()[0]\n",
    "train['category'].fillna(mode, inplace=True)\n",
    "val['category'].fillna(mode, inplace=True)\n",
    "test['category'].fillna(mode, inplace=True)\n",
    "\n",
    "# IF 5-30% MISSING & MODE/MEDIAN VARIES MEANGINFULLY ACROSS GROUPS: Group-wise mode imputation \n",
    "# categorical example: calculate mode color by brand group\n",
    "color_by_brand = train.groupby('brand')['color'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else 'Unknown').to_dict() # on TRAIN ONLY!\n",
    "train['color'] = train.apply(\n",
    "    lambda row: color_by_brand.get(row['brand'], 'Unknown') if pd.isna(row['color']) else row['color'],\n",
    "    axis=1\n",
    ")\n",
    "# Numerical example: Calculate median price per category\n",
    "price_by_category = train.groupby('category')['price'].median().to_dict()\n",
    "for df in [train, val, test]:\n",
    "    df['price'] = df.apply(\n",
    "        lambda row: price_by_category.get(row['category'], train['price'].median()) if pd.isna(row['price']) else row['price'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# IF 5-30% MISSING & MISSING MEANS \"NONE\" OR ABSENCE: Fill with zeroes or 'unknown' (e.g. num_extra_bedrooms = NaN means no extra bedrooms)\n",
    "train['num_views'].fillna(0, inplace=True) # numerical case\n",
    "val['num_views'].fillna(0, inplace=True)\n",
    "test['num_views'].fillna(0, inplace=True)\n",
    "\n",
    "train['category'].fillna('Unknown', inplace=True) # categorical case\n",
    "val['category'].fillna('Unknown', inplace=True)\n",
    "test['category'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# IF 5-30% MISSING & MISSINGNESS IS INFORMATIVE/PREDICTIVE/HAS A PATTERN: Simple median/mode imputation AND missing flag column\n",
    "train['income_missing'] = train['income'].isnull().astype(int)\n",
    "val['income_missing'] = val['income'].isnull().astype(int)\n",
    "test['income_missing'] = test['income'].isnull().astype(int)\n",
    "# fill = train['income'].median().....\n",
    "\n",
    "# IF >30% MISSING AND NOT CRTICIAL: Drop cols\n",
    "cols_to_drop = ['col1', 'col2', 'col3'] # # DROPPING COLS\n",
    "train.drop(cols_to_drop, axis=1, inplace=True)\n",
    "val.drop(cols_to_drop, axis=1, inplace=True)\n",
    "test.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# CLIPPING OUTLIERS\n",
    "lower = train['price'].quantile(0.01)\n",
    "upper = train['price'].quantile(0.99)\n",
    "\n",
    "train['price'] = train['price'].clip(lower, upper)\n",
    "val['price'] = val['price'].clip(lower, upper)\n",
    "\n",
    "##########################\n",
    "# 3.2: ENCODING\n",
    "##########################\n",
    "# IF BINARY CATEGORICAL COLUMNS: binary Label Encoding\n",
    "train['has_sunroof'] = train['has_sunroof'].map({'No': 0, 'Yes': 1})\n",
    "val['has_sunroof'] = val['has_sunroof'].map({'No': 0, 'Yes': 1})\n",
    "test['has_sunroof'] = test['has_sunroof'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# IF NATURAL ORDER: ordinal encoding\n",
    "condition_map = {'Poor': 0, 'Fair': 1, 'Good': 2, 'Excellent': 3}\n",
    "train['col'] = train['col'].map(condition_map)\n",
    "val['col'] = val['col'].map(condition_map)\n",
    "test['col'] = test['col'].map(condition_map)\n",
    "\n",
    "# IF NO ORDER, LOW CARDINALITY (<10 categories), AND USING TREE MODEL: Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = train.select_dtypes(include=['object']).columns.tolist()\n",
    "cat_cols = [c for c in cat_cols if c not in ['id', 'car_id', 'customer_id']]\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    train[col] = le.fit_transform(train[col].astype(str))\n",
    "    val[col] = le.transform(val[col].astype(str))\n",
    "    test[col] = le.transform(test[col].astype(str))\n",
    "\n",
    "# IF NO ORDER, LOW CARDINALITY (<10 categories), AND USING LINEAR MODEL: One-hot encoding\n",
    "train = pd.get_dummies(train, columns=['color'], prefix='clr')\n",
    "val = pd.get_dummies(val, columns=['color'], prefix='clr')\n",
    "test = pd.get_dummies(test, columns=['color'], prefix='clr')\n",
    "\n",
    "# IF NO ORDER, HIGH CARDINALITY (>10 categories): Target encoding\n",
    "target_means = training.groupby('col')['targetcol'].mean().to_dict() # get means on training ONLY\n",
    "\n",
    "training['col_encoded'] = training['col'].map(target_means)\n",
    "val['col_encoded'] = val['col'].map(target_means)\n",
    "test['col_encoded'] = test['col'].map(target_means)\n",
    "\n",
    "##########################\n",
    "# 3.3: SCALING\n",
    "##########################\n",
    "\n",
    "# Standardizing (Distribution of Normal(0,1)): Feature already normally dist'd. Useful for linear models--Linreg, logreg, svms, NNs, etc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_cols = ['mileage', 'horsepower', 'age']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train[feature_cols] = scaler.fit_transform(train[feature_cols])\n",
    "val[feature_cols] = scaler.transform(val[feature_cols])\n",
    "test[feature_cols] = scaler.transform(test[feature_cols])\n",
    "\n",
    "# Log Plus 1 Transformation (--> Normal dist): Feature right-skewed/skewness>1, wide range of vals, many outliers. Useful for linear models.\n",
    "train['price_log'] = np.log1p(train['price'])\n",
    "val['price_log'] = np.log1p(val['price']) # often must STANDARDIZE AFTER\n",
    "test['price_log'] = np.log1p(test['price'])\n",
    "\n",
    "# Min-Max scaling (Normalizing 0 to 1): Features have known bounds, all features are >= 0, many zeros, outliers. Useful for NNs, pixel data.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "cols_to_scale = ['mileage', 'horsepower', 'age']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train[cols_to_scale] = scaler.fit_transform(train[cols_to_scale])\n",
    "val[cols_to_scale] = scaler.transform(val[cols_to_scale])\n",
    "test[cols_to_scale] = scaler.transform(test[cols_to_scale])\n",
    "\n",
    "##########################\n",
    "# 3.4: FEATURE ENGINEERING\n",
    "##########################\n",
    "# sum\n",
    "train['total_rooms'] = train['bedrooms'] + train['bathrooms']\n",
    "val['total_rooms'] = val['bedrooms'] + val['bathrooms']\n",
    "test['total_rooms'] = test['bedrooms'] + test['bathrooms']\n",
    "\n",
    "# multiplication\n",
    "train['age_mileage'] = train['age'] * train['mileage']\n",
    "val['age_mileage'] = val['age'] * val['mileage']\n",
    "test['age_mileage'] = test['age'] * test['mileage']\n",
    "\n",
    "# division or ratios (add 1 to avoid /0)\n",
    "train['views_per_age'] = train['num_views'] / (train['age'] + 1)\n",
    "val['views_per_age'] = val['num_views'] / (val['age'] + 1)\n",
    "test['views_per_age'] = test['num_views'] / (test['age'] + 1)\n",
    "\n",
    "# date cols\n",
    "train['year'] = train['date'].dt.year # do for all 3 sets\n",
    "train['month'] = train['date'].dt.month\n",
    "train['day'] = train['date'].dt.day\n",
    "train['is_weekend'] = train['date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "train['dayofweek'] = train['date'].dt.dayofweek\n",
    "\n",
    "##########################\n",
    "# 3.5: PREPARE X AND Y FOR MODEL TRAINING\n",
    "##########################\n",
    "target_col = 'price'\n",
    "id_col = 'car_id'\n",
    "feature_cols = [c for c in train.columns if c not in [id_col, target_col]]\n",
    "\n",
    "# IF VAL SET PROVIDED: simple x-y split\n",
    "X_train = train[feature_cols]\n",
    "y_train = train[target_col]\n",
    "\n",
    "X_val = val[feature_cols]\n",
    "y_val = val[target_col]\n",
    "\n",
    "X_test = test[feature_cols]\n",
    "test_ids = test[id_col]\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "\n",
    "# IF VAL SET NOT PROVIDED: 80/20 train-test SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train.drop(['id', 'target'], axis=1)\n",
    "y = train['target']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) # create validation set\n",
    "\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "test_ids = test['id']\n",
    "\n",
    "# IF CLASSIFICATION PROBLEM: STRATIFIED SPLIT\n",
    "X = train.drop(['id', 'target'], axis=1)\n",
    "y = train['target']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y  # Keep class balance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1ca1a-1273-4827-928e-4b4cdb5d4ed1",
   "metadata": {},
   "source": [
    "### 4. Train multiple models (predict on validation set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e6cce-64af-4f7c-b8e5-a1ac1437569f",
   "metadata": {},
   "source": [
    "#### 4.0: Model Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75226e68-a018-466d-85d9-2805aa87f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# LINEAR REGRESSION FAMILY\n",
    "########################################\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "# Simple Linear Regression (no regularization)\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Ridge (L2 regularization) - most common\n",
    "ridge = Ridge(alpha=1.0)\n",
    "\n",
    "# Lasso (L1 regularization) - feature selection\n",
    "lasso = Lasso(alpha=1.0)\n",
    "\n",
    "'''Assumptions:\n",
    "1. linearity (straight-line relationship between x and y) | Residuals vs. fitted values plot \n",
    "2. independence (errors unrelated) | depends how data collected\n",
    "3. homoscedasticity (variance of errors/residuals should be constant across all vals of indep var) | plot(x=fitted vals, y= stdized residuals)\n",
    "4. Normality (residual errors should be normally distributed centered around 0) | qqplot of residuals, points along qqline\n",
    "'''\n",
    "# CHECK IF LINEAR MODEL IS APPROPRIATE\n",
    "\n",
    "# 1. Check for linearity (correlation with target)\n",
    "print(train[numeric_cols].corr()['target'].sort_values(ascending=False))\n",
    "\n",
    "# 2. Check for skewness (should be < 1.0)\n",
    "for col in numeric_cols:\n",
    "    skew = train[col].skew()\n",
    "    if abs(skew) > 1.0:\n",
    "        print(f\"{col}: skewness={skew:.2f} - consider log transform\")\n",
    "\n",
    "# 3. Check multicollinearity\n",
    "corr_matrix = train[numeric_cols].corr()\n",
    "high_corr = (corr_matrix.abs() > 0.8) & (corr_matrix != 1.0)\n",
    "if high_corr.any().any():\n",
    "    print(\"High multicollinearity detected - use Ridge\")\n",
    "\n",
    "########################################\n",
    "# LOGISTIC REGRESSION\n",
    "########################################\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    max_iter=1000,           # Increase if convergence warning\n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # Use if imbalanced\n",
    ")\n",
    "'''Assumptions:\n",
    "1. binary/multinomial outcome | target is categorical var\n",
    "2. Linearity with log-odds | Assumes: log(P/(1-P)) is linear in X, Fix by: Add polynomial features, interactions\n",
    "3. independence (errors unrelated) | depends how data collected\n",
    "4. homoscedasticity (variance of errors/residuals should be constant across all vals of indep var) | plot(x=fitted vals, y= stdized residuals)\n",
    "5. Large Sample Size (At least 10 samples per feature per class) | Need n >> p (many more samples than features), fix by: L2 regularization (reduce feats, get more data)\n",
    "'''\n",
    "#########################################\n",
    "# DECISION TREE\n",
    "########################################\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeRegressor(\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    random_state=42\n",
    ")\n",
    "'''\n",
    "✅ Assumptions: NONE! \n",
    "✅ No linearity assumption, normality assumption, or homoscedasticity assumption\n",
    "✅ Can handle non-linear patterns\n",
    "✅ Can handle feature interactions automatically\n",
    "✅ No scaling needed\n",
    "'''\n",
    "#########################################\n",
    "# ENSEMBLE METHODS\n",
    "#########################################\n",
    "# RANDOM FOREST (uses bagging/boostrap aggregation)\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\"\"\" How it works:\n",
    "1. Create N bootstrap samples (random sampling with replacement)\n",
    "2. Train independent model on each sample\n",
    "3. Average predictions (parallel)\n",
    "Models trained independently in parallel, with goal: Reduce variance (overfitting)\n",
    "\"\"\"\n",
    "# GRADIENT BOOSTING (sequential learning)\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "'''\n",
    "How it works:\n",
    "1. Train model on data\n",
    "2. Train next model on errors of previous model\n",
    "3. Combine predictions with weights (sequential)\n",
    "Models trained sequentially, with goal: Reduce bias (underfitting)\n",
    "'''\n",
    "\n",
    "'''\n",
    "Overall ensemble method Assumptions: NONE! \n",
    "✅ No linearity assumption, normality assumption, or homoscedasticity assumption\n",
    "✅ Can handle non-linear patterns\n",
    "✅ Can handle feature interactions automatically\n",
    "✅ No scaling needed\n",
    "'''\n",
    "#########################################\n",
    "# DISTANCE-BASED MODELS\n",
    "#########################################\n",
    "\n",
    "# K-NEAREST NEIGHBORS\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsRegressor(\n",
    "    n_neighbors=5,           # Number of neighbors\n",
    "    weights='distance',      # Weight by distance\n",
    "    metric='euclidean'       # Distance metric\n",
    ")\n",
    "'''\n",
    "Assumptions:\n",
    "1. Similar Points → Similar Outcomes (Nearby points in feature space have similar targets) | Distance in feature space is meaningful\n",
    "2. Features are Scaled (All features must be on same scale, If violated: Large-scale features dominate) |Fix: ALWAYS use StandardScaler or MinMaxScaler\n",
    "3. Low Dimensionality (Works with (< 20 features)) | \"Curse of dimensionality\" - IF VIOLATED, All points equally distant, poor performance. Fix by: Dimensionality reduction (PCA), feature selection\n",
    "'''\n",
    "# SUPPORT VECTOR MACHINES\n",
    "from sklearn.svm import SVR, SVC\n",
    "\n",
    "# Regression\n",
    "svr = SVR(\n",
    "    kernel='rbf',            # 'linear', 'poly', 'rbf'\n",
    "    C=1.0,                   # Regularization\n",
    "    gamma='scale'            # Kernel coefficient\n",
    ")\n",
    "\n",
    "# Classification\n",
    "svc = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    random_state=42\n",
    ")\n",
    "'''\n",
    "Assumptions:\n",
    "1. Data is Scaled | Fix: ALWAYS use StandardScaler\n",
    "2. Kernel Choice Matters | Linear kernel: Assumes linear separability, RBF kernel: Can handle non-linear patterns, Polynomial: Can handle polynomial relationships\n",
    "3. Not Too Many Features (Prefers: n_samples > n_features) | fix: feature selection, regularization w/ c param\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3a493-ed3e-4c94-90ce-19642dbb24c2",
   "metadata": {},
   "source": [
    "#### 4.1 Model Selection step-by-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b35f89c-f941-4876-82ec-321a7d346a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n4.1: Model Selection\\n├── REGRESSION\\n│   ├── Small dataset (<1000): Ridge → RF → GB\\n│   └── Large dataset (>1000): RF → GB\\n└── CLASSIFICATION\\n    ├── Balanced classes: RF → GB → LogReg\\n    └── Imbalanced classes: RF(class_weight) → SMOTE\\n\\n4.2: Hyperparameter Tuning\\n├── >5 min → GridSearchCV\\n├── 2-5 min → Manual tuning\\n└── <2 min → Default params\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "4.1: Model Selection\n",
    "SUPERVISED LEARNING\n",
    "├── REGRESSION (continuous target)\n",
    "│   ├── Linear Models \n",
    "│   ├── Tree-based Models \n",
    "│   ├── Distance-based Models\n",
    "│   ├── Small dataset (<1000): Ridge → RF → GB\n",
    "│   └── Large dataset (>1000): RF → GB\n",
    "└── CLASSIFICATION (categorical target)\n",
    "    ├── Linear Models \n",
    "    ├── Tree-based Models \n",
    "    ├── Distance-based Models\n",
    "    ├── Balanced classes: RF → GB → LogReg\n",
    "    └── Imbalanced classes: RF(class_weight) → SMOTE\n",
    "\n",
    "4.2: Hyperparameter Tuning\n",
    "├── >5 min → GridSearchCV\n",
    "├── 2-5 min → Manual tuning\n",
    "└── <2 min → Default params\n",
    "'''\n",
    "##########################\n",
    "# 4.1: Regression\n",
    "##########################\n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# RANDOM FOREST\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100, # more trees = better performance, less variance/overfitting, BUT slower training\n",
    "    max_depth=10, # hgiher depth = capture complex patterns, better on training, BUT risk of overfitting, slower training\n",
    "    min_samples_split=5, # higher = less overfitting, faster training, BUT might underfit/high bias\n",
    "    min_samples_leaf=2, # hgiher = smoother predicitions, less overfitting, BUT can underfit\n",
    "    random_state=42,\n",
    "    n_jobs=-1 # number CPU cores to use, -1 is ALWAYS use all cores\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f\"RandomForest MAE: ${mae:,.2f}\")\n",
    "\n",
    "# try different tree depths\n",
    "for depth in [5, 10, 15, 20]: \n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=depth, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    mae = mean_absolute_error(y_val, rf.predict(X_val))\n",
    "    print(f\"depth={depth}: MAE=${mae:,.2f}\")\n",
    "\n",
    "# GRADIENT BOOSTING\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(\n",
    "    n_estimators=100, # more trees = better training performance, BUT risk overfitting, slower training, \n",
    "    learning_rate=0.1, # smaller step size for each tree's contribution = better generalization, less overfitting, need more trees to compensate\n",
    "    max_depth=5, # deeper tree = learn complex patterns, BUT hgih overfit risk, slower training\n",
    "    min_samples_split=5, # sams as RF\n",
    "    subsample = 1, # lower subsample = adds randomness (like bagging), less overfitting, faster training, \n",
    "    random_state=42\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred = gb.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f\"GradientBoosting MAE: ${mae:,.2f}\")\n",
    "\n",
    "# try different learnign rates\n",
    "for lr in [0.01, 0.05, 0.1, 0.2]:\n",
    "    gb = GradientBoostingRegressor(n_estimators=100, learning_rate=lr, random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "    mae = mean_absolute_error(y_val, gb.predict(X_val))\n",
    "    print(f\"lr={lr}: MAE=${mae:,.2f}\")\n",
    "    \n",
    "# RIDGE (needs scaling!)\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred = ridge.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f\"Ridge MAE: ${mae:,.2f}\")\n",
    "\n",
    "# LASSO\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=1.0)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred = lasso.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f\"Lasso MAE: ${mae:,.2f}\")\n",
    "\n",
    "#XGBOOST\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f\"XGBoost MAE: ${mae:,.2f}\")\n",
    "\n",
    "# Light GBM\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgb = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "lgb.fit(X_train, y_train)\n",
    "y_pred = lgb.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f\"LightGBM MAE: ${mae:,.2f}\")\n",
    "\n",
    "##########################\n",
    "# 4.2: Classification (metric: accuracy, precision, recall, f1)\n",
    "##########################\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000, \n",
    "                            class_weight='balanced', # important if class imbalance\n",
    "                            random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"LogisticRegression Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Random Forest Classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "rf = RandomForestClassifier(\n",
    "    # ...\n",
    "    class_weight='balanced',  # DIFFERENT: Important for imbalanced data!\n",
    "    # ...\n",
    "    )\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred) # DIFFERENT: accuracy metric\n",
    "print(f\"RandomForest Accuracy: {acc:.4f}\")\n",
    "    \n",
    "# Gradient Boosting Classifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# ...\n",
    "acc = accuracy_score(y_val, y_pred) # DIFFERENT: accuracy meyric\n",
    "print(f\"GradientBoosting Accuracy: {acc:.4f}\")\n",
    "\n",
    "# XGBOOST  CLASSIFIER\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"XGBoost Accuracy: {acc:.4f}\")\n",
    "\n",
    "# SVMs (NEED SCALING!)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', C=1.0, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"SVM Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a98f3-f742-4466-a417-dab428631f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# REGRESSION: COMPARE 3 MODELS QUICKLY\n",
    "##########################\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'RF': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'GB': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'Ridge': Ridge(alpha=1.0)\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    results[name] = [mae,rmse,r2]\n",
    "    print(f\"{name}: MAE=${mae:,.2f}, RMSE=${rmse:,.2f},r2=${r2:,.2f}\")\n",
    "\n",
    "# Get best model\n",
    "best_name = min(results, key=results.get)\n",
    "best_model = models[best_name]\n",
    "print(f\"\\n✅ Best model: {best_name} (MAE=${results[best_name]:,.2f})\")\n",
    "\n",
    "##########################\n",
    "# CLASSIFICATION: COMPARE 3 MODELS QUICKLY\n",
    "##########################\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "models = {\n",
    "    'RF': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'GB': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'LogReg': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    prec = precision_score(y_val, y_pred, average='weighted')\n",
    "    rec = recall_score(y_val, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "    \n",
    "    results[name] = [acc,prec,rec,f1]\n",
    "    print(f\"{name:<10} {acc:>10.4f} {prec:>10.4f} {rec:>10.4f} {f1:>10.4f}\")\n",
    "\n",
    "# Get best\n",
    "best_name = max(results, key=results.get)\n",
    "best_model = models[best_name]\n",
    "print(f\"\\n Best model: {best_name} (Accuracy={results[best_name]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f1ef0-6324-45bd-99c8-d7af44100328",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# HYPERPARAMETER TUNING\n",
    "##########################\n",
    "# Random Forest (regression with Grid Search)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5] # dont use this in classification scenarios, otherwise process is same\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "grid = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best params: {grid.best_params_}\")\n",
    "print(f\"Best score: ${-grid.best_score_:,.2f}\")\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f\"Validation MAE: ${mae:,.2f}\")\n",
    "\n",
    "# same for GB\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "grid = GridSearchCV(gb, param_grid, cv=3, scoring='neg_mean_absolute_error')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best params: {grid.best_params_}\")\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# RANDOM FOREST REGRESSION WITH RANDOMIZED SEARCH (faster than grid search)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf, \n",
    "    param_dist, \n",
    "    n_iter=10,  # Try 10 random combinations\n",
    "    cv=3, \n",
    "    scoring='neg_mean_absolute_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best params: {random_search.best_params_}\")\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# MANUAL TUNING (2d grid here)\n",
    "best_mae = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "for n in [50, 100, 150]: # tuning n_estimators\n",
    "    for depth in [5, 10, 15]: # tuning max_depth\n",
    "        rf = RandomForestRegressor(n_estimators=n, max_depth=depth, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_train, y_train)\n",
    "        mae = mean_absolute_error(y_val, rf.predict(X_val))\n",
    "        print(f\"n={n}, depth={depth}: MAE=${mae:,.2f}\")\n",
    "        \n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_params = {'n_estimators': n, 'max_depth': depth}\n",
    "\n",
    "print(f\"\\n✅ Best params: {best_params}, MAE=${best_mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b1eb0-74d8-440e-b9c5-de22a2ec749a",
   "metadata": {},
   "source": [
    "### 5. Choose best model & Re-train on full data (train + val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa0423-2749-4958-a364-b625ec2d9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5.1: Validation \n",
    "├── Val set provided → Use it, retrain on train+val\n",
    "└── No val set → Create 80/20 split or CV\n",
    "\n",
    "5.2: Final Training\n",
    "└── Always combine train + val before final predictions\n",
    "\n",
    "5.3: Output Validation\n",
    "├── Format checks\n",
    "├── Value range checks\n",
    "└── File format checks\n",
    "'''\n",
    "##########################\n",
    "# 5.1: Cross-Val (if validation set not provided and dataset is tiny)\n",
    "##########################\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "scores = -scores  # Convert to positive\n",
    "\n",
    "print(f\"CV MAE scores: {scores}\")\n",
    "print(f\"Mean MAE: ${scores.mean():,.2f}\")\n",
    "print(f\"Std MAE: ${scores.std():,.2f}\")\n",
    "\n",
    "# multiple models with Cross-Val\n",
    "models = {\n",
    "    'RF': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'GB': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "    scores = -scores\n",
    "    print(f\"{name}: Mean MAE=${scores.mean():,.2f} (+/- ${scores.std():,.2f})\")\n",
    "\n",
    "##########################\n",
    "# STRATIFIED K-FOLD CROSS-VAL (for classification)\n",
    "##########################\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(rf, X_train, y_train, cv=skf, scoring='accuracy')\n",
    "\n",
    "print(f\"CV Accuracy scores: {scores}\")\n",
    "print(f\"Mean Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "# Feature Importance\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(10))\n",
    "\n",
    "##########################\n",
    "# 5.2: FULL MODEL EVALUATION/METRICS\n",
    "##########################\n",
    "# ALL REGRESSION METRICS\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "# ALL CLASSIFICATION METRICS\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "prec = precision_score(y_val, y_pred, average='weighted')\n",
    "rec = recall_score(y_val, y_pred, average='weighted')\n",
    "f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# BINARY CLASSIFICATION METRICS\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_proba = best_model.predict_proba(X_val)[:, 1]  # Probabilities for class 1\n",
    "roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# 5.3: FINAL TRAINING: RE-TRAIN BEST MODEL ON TRAIN + VAL SET\n",
    "# RE-DO FULL METRICS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
